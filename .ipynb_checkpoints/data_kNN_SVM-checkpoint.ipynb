{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: Data Mining Exercise 5  - kNN, SVM\n",
    "exercise-number: 5\n",
    "other-links:\n",
    "  - text: Aufgabenblatt 5 \n",
    "    href: \"../../../../_aufgaben/Aufgabenblaetter/5/05_Customer_data_kNN_SVM_student.pdf\"\n",
    "    icon: file-pdf\n",
    "code-links:\n",
    "  - text: Aufgabenblatt 5 \n",
    "    href: \"../../../../_aufgaben/Aufgabenblaetter/5/05_Customer_data_kNN_SVM_student.ipynb\"\n",
    "    type: \"application/octet-stream\"\n",
    "    icon: file-code\n",
    "downloads: [ipynb]\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 5 - kNN & SVM\n",
    "Necessary libraries are being loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Extract from the data preparation\n",
    "Explain with comments in the code what is performed here by the individual commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "df = pd.read_csv('prepared_data.csv')\n",
    "\n",
    "df['high revenue'] = np.where(df['total_sum'] > 300, 1, 0)\n",
    "\n",
    "dfFilter = df[['gender','age_first_order','user_agent_brand',\n",
    "         'user_agent_os', 'campaign', 'pages_visited_avg','high revenue']]\n",
    "\n",
    "dfCopy = dfFilter.copy()\n",
    "\n",
    "dfCopy['gender'] = dfCopy['gender'].fillna(dfCopy['gender'].mode()[0])\n",
    "dfCopy['age_first_order'] = dfCopy['age_first_order'].fillna(dfFilter['age_first_order'].mode()[0])\n",
    "\n",
    "dfCopy[\"campaign\"] = dfCopy[\"campaign\"].astype(int)\n",
    "\n",
    "dfPrepared = dfCopy.copy()\n",
    "dfPrepared.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 One-Hot-Encoding & Normalization\n",
    "Now carry out binary encoding for all relevant features and save the result in the dfTrans data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the correlations with the heat map. Remove characteristics that correlate 100% with others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.heatmap(dfTrans.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all non-binary attributes must be normalized. This time minmax_scale should be used for this. Use comments to explain the function of the individual lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import minmax_scale\n",
    "dfnorm = dfTrans.copy()\n",
    "\n",
    "scaled = minmax_scale(dfnorm[['age_first_order','pages_visited_avg']], feature_range = (0, 1))\n",
    "\n",
    "dfnorm['age_first_order'] = scaled[:,0] \n",
    "dfnorm['pages_visited_avg'] = scaled[:,1]\n",
    "\n",
    "dfnorm[['age_first_order', 'pages_visited_avg']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "have a short look at our prepared dataframe. all numeric columns with a natural order are standardised and scaled. all columns without a natural order are one-hot encoded. Our target column ‘high revenue’ is also binary. we want it that way because we perform a classification and either assign a data record to high revenue (1) or not (0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfnorm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Training & evaluation of the models\n",
    "\n",
    "In the following, the algorithms kNN and SVM are to be trained and tested. For this purpose, a data split of 70 % (training data) to 30 % (test data) must be carried out in advance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x = dfnorm.drop(['high revenue'], axis = 1)\n",
    "y = dfnorm['high revenue'].astype(int)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y,\n",
    "                                    random_state = 101, stratify = y, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A1: k-Nearest Neighbors (kNN)\n",
    "Train the kNN on the training data and evaluate on the test data. Test different values for k."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following visualisation will help you to try out good ks. Here you can see again the change in the error rate with different k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "error_rate = []\n",
    "\n",
    "k_range = range(1, 30)\n",
    "\n",
    "for k in k_range:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(x_train, y_train)\n",
    "    \n",
    "    y_pred = knn.predict(x_test)\n",
    "    \n",
    "    error_rate.append(1 - accuracy_score(y_test, y_pred))  \n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(k_range, error_rate, color='blue', linestyle='dashed', marker='o', \n",
    "         markerfacecolor='red', markersize=10)\n",
    "plt.title('Elbow method for KNN')\n",
    "plt.xlabel('Number of neighbours: k')\n",
    "plt.ylabel('Error rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a quick look at the distribution of the predicted classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Count how many data points were classified as 0 and how many as 1\n",
    "unique, counts = np.unique(y_pred, return_counts=True)\n",
    "\n",
    "# Calculate percentage distribution\n",
    "total = len(y_pred)\n",
    "percentages = (counts / total) * 100\n",
    "\n",
    "# Output of the results\n",
    "for u, count, percentage in zip(unique, counts, percentages):\n",
    "    print(f'Class {u}: {count} Datapoints ({percentage:.2f}%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This distribution also becomes apparent when we visualise the data points. Note: The n-dimensional space is reduced to 3 dimensions using PCA (theoretical Background not part of the class!) in order to make visualisation possible . PCA (Principal Component Analysis) does not simply select three existing features, but creates new features (the so-called principal components), which are linearly combined variables of the original features. The aim is to reduce the dimensions in such a way that the maximum possible variance is retained in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Dimension reduction to 3 dimensions using PCA\n",
    "pca = PCA(n_components=3)\n",
    "x_train_pca = pca.fit_transform(x_train)\n",
    "x_test_pca = pca.transform(x_test)\n",
    "\n",
    "# Visualisation of the test data points\n",
    "fig = plt.figure(figsize=(10, 7))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Create a scatter plot with the test data and the prediction (y_pred) as colours\n",
    "scatter = ax.scatter(x_test_pca[:, 0], x_test_pca[:, 1], x_test_pca[:, 2], \n",
    "                     c=y_pred, cmap='viridis', s=50, alpha=0.8)\n",
    "\n",
    "# Add colour bars to display the classes\n",
    "legend1 = ax.legend(*scatter.legend_elements(), title=\"classes\")\n",
    "ax.add_artist(legend1)\n",
    "\n",
    "# Axis labelling\n",
    "ax.set_xlabel('PCA 1')\n",
    "ax.set_ylabel('PCA 2')\n",
    "ax.set_zlabel('PCA 3')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model evaluation\n",
    "Then use the scorer or other addressed methods to evaluate the Confusion Matrix and the accuracy of your models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "\n",
    "tpr = tp / (tp + fn)\n",
    "print(f'True Positive Rate (TPR): {tpr:.2f}')\n",
    "\n",
    "fpr = fp / (fp + tn)\n",
    "print(f'False Positive Rate (FPR): {fpr:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "y_proba = knn_model.predict_proba(x_test)[:, 1]\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_proba)\n",
    "\n",
    "auc_score = roc_auc_score(y_test, y_proba)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='blue', label=f'ROC curve (AUC = {auc_score:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')  \n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A1.1 kNN with shepards method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For comparison, look at the same algorithm but with Shepard's method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn_model = KNeighborsClassifier(n_neighbors=23, weights='distance')\n",
    "knn_model.fit(x_train, y_train)\n",
    "\n",
    "y_pred = knn_model.predict(x_test)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A2: Support Vector Machine\n",
    "Train an SVM on the training data and evaluate on the test data. Test with the parameter transfer (kernel = 'rbf', gamma = 'scale')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model evaluation\n",
    "Then use the scorer or other addressed methods to evaluate the Confusion Matrix and the accuracy of your models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Count how many data points were classified as 0 and how many as 1\n",
    "unique, counts = np.unique(y_pred, return_counts=True)\n",
    "\n",
    "# Calculate percentage distribution\n",
    "total = len(y_pred)\n",
    "percentages = (counts / total) * 100\n",
    "\n",
    "# Output of the results\n",
    "for u, count, percentage in zip(unique, counts, percentages):\n",
    "    print(f'Class {u}: {count} Datapoints ({percentage:.2f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "\n",
    "tpr = tp / (tp + fn)\n",
    "print(f'True Positive Rate (TPR): {tpr:.2f}')\n",
    "\n",
    "fpr = fp / (fp + tn)\n",
    "print(f'False Positive Rate (FPR): {fpr:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Activate SVC model with probabilities\n",
    "svm_model = SVC(kernel='rbf', gamma='scale', random_state=1, probability=True)\n",
    "svm_model.fit(x_train, y_train)\n",
    "\n",
    "# Predict probabilities (only for the positive class)\n",
    "y_prob = svm_model.predict_proba(x_test)[:, 1]\n",
    "\n",
    "# Calculate ROC curve and AUC\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
    "roc_auc = roc_auc_score(y_test, y_prob)\n",
    "\n",
    "# Plot the ROC-Curve\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve for SVM Model')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of the algorithms\n",
    "Assess the quality of the two algorithms by evaluating the results. Which one would you use?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Optimization with GridSearch ( Preview)\n",
    "Gridsearch is a method for hyperparameter optimization. Gridsearch iteratively tries out all combinations of the selected parameters. The combination with the highest score is then output.\n",
    "\n",
    "For the sake of simplicity, cross-validation is not used in this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A1: SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_model = SVC(random_state = 1)\n",
    "param_grid1 = {\"kernel\": [\"poly\", \"rbf\", \"sigmoid\"], \n",
    "              \"gamma\": [\"scale\",\"auto\"] }\n",
    "gridSearch1 = GridSearchCV(estimator = svm_model, param_grid = param_grid1)\n",
    "gridSearch1.fit(x_train, y_train)\n",
    "\n",
    "results1 = pd.DataFrame(gridSearch1.cv_results_)\n",
    "\n",
    "params1 = results1.loc[results1['rank_test_score'].idxmax()]\n",
    "params1['params']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to train the model with the best parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now look at the ConfusionMatrix again and compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "\n",
    "tpr = tp / (tp + fn)\n",
    "print(f'True Positive Rate (TPR): {tpr:.2f}')\n",
    "\n",
    "fpr = fp / (fp + tn)\n",
    "print(f'False Positive Rate (FPR): {fpr:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_model = SVC(kernel='sigmoid', gamma='scale', random_state=1, probability=True)\n",
    "svm_model.fit(x_train, y_train)\n",
    "\n",
    "y_prob = svm_model.predict_proba(x_test)[:, 1]\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
    "roc_auc = roc_auc_score(y_test, y_prob)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve for SVM Model')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A1: k-Nearest Neighbours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now convert GridSearch for k-Nearest Neighbors and select different values for metric and n_neighbors in param_grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# space "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now look at the ConfusionMatrix again and compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "\n",
    "tpr = tp / (tp + fn)\n",
    "print(f'True Positive Rate (TPR): {tpr:.2f}')\n",
    "\n",
    "fpr = fp / (fp + tn)\n",
    "print(f'False Positive Rate (FPR): {fpr:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "y_proba = knn_model.predict_proba(x_test)[:, 1]\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_proba)\n",
    "\n",
    "auc_score = roc_auc_score(y_test, y_proba)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='blue', label=f'ROC curve (AUC = {auc_score:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')  \n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "although our true positive rate has improved, the ROC-curve has deteriorated. Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# space"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
