{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: Data Mining Exercise 7  - trees\n",
    "exercise-number: 7\n",
    "other-links:\n",
    "  - text: Aufgabenblatt 7\n",
    "    href: \"../../../../_aufgaben/Aufgabenblaetter/7/07_Customer_data_DT_boosting_regression_student.pdf\"\n",
    "    icon: file-pdf\n",
    "code-links:\n",
    "  - text: Aufgabenblatt 7\n",
    "    href: \"../../../../_aufgaben/Aufgabenblaetter/7/07_Customer_data_DT_boosting_regression_student.ipynb\"\n",
    "    type: \"application/octet-stream\"\n",
    "    icon: file-code\n",
    "downloads: [ipynb]\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 7 - DT, gradient boosting and random forest for regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the customer data again. This time we do not want to perform a classification according to high_revenue, but instead directly predict the numerical value turnover (total_sum)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>weekday</th>\n",
       "      <th>daytime</th>\n",
       "      <th>isHoliday</th>\n",
       "      <th>distance</th>\n",
       "      <th>count</th>\n",
       "      <th>startClusterName</th>\n",
       "      <th>startClusterZip</th>\n",
       "      <th>startClusterID</th>\n",
       "      <th>endClusterName</th>\n",
       "      <th>endClusterZip</th>\n",
       "      <th>endClusterID</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Di</td>\n",
       "      <td>7</td>\n",
       "      <td>Keine Ferien</td>\n",
       "      <td>3</td>\n",
       "      <td>15</td>\n",
       "      <td>Hunedoara</td>\n",
       "      <td>71171</td>\n",
       "      <td>3254026000002</td>\n",
       "      <td>Hunedoara</td>\n",
       "      <td>71171</td>\n",
       "      <td>3254026000007</td>\n",
       "      <td>2021</td>\n",
       "      <td>3</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mi</td>\n",
       "      <td>17</td>\n",
       "      <td>Keine Ferien</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>Turda</td>\n",
       "      <td>80982</td>\n",
       "      <td>3241013050002</td>\n",
       "      <td>Turda</td>\n",
       "      <td>80982</td>\n",
       "      <td>3241013050008</td>\n",
       "      <td>2021</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sa</td>\n",
       "      <td>11</td>\n",
       "      <td>Keine Ferien</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>Turda</td>\n",
       "      <td>80982</td>\n",
       "      <td>3241013050007</td>\n",
       "      <td>Turda</td>\n",
       "      <td>80982</td>\n",
       "      <td>3241013030001</td>\n",
       "      <td>2021</td>\n",
       "      <td>3</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fr</td>\n",
       "      <td>13</td>\n",
       "      <td>Keine Ferien</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>Bran</td>\n",
       "      <td>91157</td>\n",
       "      <td>3254028001012</td>\n",
       "      <td>Bran</td>\n",
       "      <td>91157</td>\n",
       "      <td>3254028001004</td>\n",
       "      <td>2021</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fr</td>\n",
       "      <td>18</td>\n",
       "      <td>Keine Ferien</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>Bran</td>\n",
       "      <td>91157</td>\n",
       "      <td>3254028001003</td>\n",
       "      <td>Turda</td>\n",
       "      <td>80982</td>\n",
       "      <td>3241013070001</td>\n",
       "      <td>2021</td>\n",
       "      <td>3</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  weekday  daytime     isHoliday  distance  count startClusterName  \\\n",
       "0      Di        7  Keine Ferien         3     15        Hunedoara   \n",
       "1      Mi       17  Keine Ferien         0     10            Turda   \n",
       "2      Sa       11  Keine Ferien         3     10            Turda   \n",
       "3      Fr       13  Keine Ferien         0     15             Bran   \n",
       "4      Fr       18  Keine Ferien         7     10             Bran   \n",
       "\n",
       "   startClusterZip  startClusterID endClusterName  endClusterZip  \\\n",
       "0            71171   3254026000002      Hunedoara          71171   \n",
       "1            80982   3241013050002          Turda          80982   \n",
       "2            80982   3241013050007          Turda          80982   \n",
       "3            91157   3254028001012           Bran          91157   \n",
       "4            91157   3254028001003          Turda          80982   \n",
       "\n",
       "    endClusterID  year  month  total  \n",
       "0  3254026000007  2021      3     45  \n",
       "1  3241013050008  2021      3      0  \n",
       "2  3241013030001  2021      3     30  \n",
       "3  3254028001004  2021      3      0  \n",
       "4  3241013070001  2021      3     70  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# space\n",
    "df = pd.read_csv('prepared_data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 Handling missing values\n",
    "First of all, we start with the treatment of missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "dfFilter = df[['gender','age_first_order','user_agent_brand',\n",
    "         'user_agent_os', 'campaign', 'pages_visited_avg', 'total_sum']]\n",
    "\n",
    "dfCopy = dfFilter.copy()\n",
    "\n",
    "dfCopy['gender'] = dfCopy['gender'].fillna(dfCopy['gender'].mode()[0])\n",
    "dfCopy['age_first_order'] = dfCopy['age_first_order'].fillna(df['age_first_order'].mode()[0])\n",
    "dfCopy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Coding of relevant attributes\n",
    "In the documentation of the algorithms to be used, it is clear that the data must be numerical. Try to understand this. Why is a label encoder sufficient here? Why does it not need to be normed?\n",
    "\n",
    "https://scikit-learn.org/stable/modules/classes.html#module-sklearn.tree\n",
    "\n",
    "https://scikit-learn.org/stable/modules/classes.html#module-sklearn.ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "labelenc = preprocessing.LabelEncoder()\n",
    "labelenc.fit(dfCopy.gender)\n",
    "dfCopy['gender'] = labelenc.transform(dfCopy.gender)\n",
    "\n",
    "labelenc.fit(dfCopy.user_agent_os)\n",
    "dfCopy['user_agent_os'] = labelenc.transform(dfCopy.user_agent_os)\n",
    "\n",
    "labelenc.fit(dfCopy.user_agent_brand)\n",
    "dfCopy['user_agent_brand'] = labelenc.transform(dfCopy.user_agent_brand)\n",
    "\n",
    "dfCopy[\"campaign\"] = dfCopy[\"campaign\"].astype(int)\n",
    "dfCopy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, the specialist department would like to discretize the age, as this personal characteristic will soon only be queried in the following intervals ['up to 20', '20-30', '30-40', '40-50', '50-60', '60-70', 'over 70' ] for reasons of data protection and acceptance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 Training the algorithms with k-fold cross-validation\n",
    "In the following, three tree-based methods from the lecture \"Supervised Methods Part 2\" will be applied. For this purpose, a 10-fold cross-validation is to be applied and then the model is to be evaluated using statistical key figures.\n",
    "In each case, use the function cross_val_predict(model, x,y,cv=10) for the cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict, cross_validate\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "x = dfPrepared.drop(['total_sum'], axis = 1)\n",
    "y = dfPrepared['total_sum'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A1: Decision Tree Regressor\n",
    "Train with the use of cross_val_predict(XYregressor, x,y,cv=10) at https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_predict.html?highlight=cross_val_predict#sklearn.model_selection.cross_val_predict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metrics for the evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train as above.mae = metrics.mean_absolute_error(y, y_pred)\n",
    "mse = metrics.mean_squared_error(y, y_pred)\n",
    "rmse = np.sqrt(mse)  \n",
    "r2 = metrics.r2_score(y,y_pred)\n",
    "\n",
    "print(\"Ergebnisse von sklearn.metrics:\\n\")\n",
    "print(\"R-Squared:\", r2)\n",
    "print()\n",
    "\n",
    "print(\"MAE:\",mae)\n",
    "print(\"MSE:\", mse)\n",
    "print(\"RMSE:\", rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "after training the algorithm, you can  have a look at the important features for this conrete trained algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "treeRegressor.fit(x, y)\n",
    "\n",
    "feature_importances = treeRegressor.feature_importances_\n",
    "\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': x.columns,\n",
    "    'Importance': feature_importances\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(importance_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "returns a list of importance for each feature, with higher values having a higher importance for the predictions.This allows you to see which features the decision tree considers important and you can use these findings for further analyses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A1.1: Decision Tree Regressor - additional parameters\n",
    "Pruning is a technique used in decision trees to simplify the tree and avoid overfitting. \n",
    "Rules for pre-pruning are, for example, a maximum tree depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "treeRegressor = DecisionTreeRegressor(min_samples_split = 10, min_samples_leaf = 5,  max_depth=10, max_leaf_nodes=50 )\n",
    "y_pred = cross_val_predict(treeRegressor, x, y, cv = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae = metrics.mean_absolute_error(y, y_pred)\n",
    "mse = metrics.mean_squared_error(y, y_pred)\n",
    "rmse = np.sqrt(mse)  \n",
    "r2 = metrics.r2_score(y,y_pred)\n",
    "\n",
    "print(\"Results from sklearn.metrics:\\n\")\n",
    "print(\"R-Squared:\", r2)\n",
    "print()\n",
    "\n",
    "print(\"MAE:\",mae)\n",
    "print(\"MSE:\", mse)\n",
    "print(\"RMSE:\", rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "treeRegressor.fit(x, y)\n",
    "\n",
    "feature_importances = treeRegressor.feature_importances_\n",
    "\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': x.columns,\n",
    "    'Importance': feature_importances\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(importance_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A2: Random Forest Regressor\n",
    "Train the same way as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae = metrics.mean_absolute_error(y, y_pred)\n",
    "mse = metrics.mean_squared_error(y, y_pred)\n",
    "rmse = np.sqrt(mse)  \n",
    "r2 = metrics.r2_score(y,y_pred)\n",
    "\n",
    "print(\"Ergebnisse von sklearn.metrics:\\n\")\n",
    "print(\"R-Squared:\", r2)\n",
    "print()\n",
    "\n",
    "print(\"MAE:\",mae)\n",
    "print(\"MSE:\", mse)\n",
    "print(\"RMSE:\", rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forestRegressor.fit(x, y)\n",
    "\n",
    "feature_importances = forestRegressor.feature_importances_\n",
    "\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': x.columns,\n",
    "    'Importance': feature_importances\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(importance_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A2.1: Random Forest Regressor - additional parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "forestRegressor = RandomForestRegressor(min_samples_split = 10, min_samples_leaf = 5, n_estimators = 100, max_depth=10, max_leaf_nodes=50 )\n",
    "\n",
    "y_pred = cross_val_predict(forestRegressor, x, y, cv = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae = metrics.mean_absolute_error(y, y_pred)\n",
    "mse = metrics.mean_squared_error(y, y_pred)\n",
    "rmse = np.sqrt(mse)  \n",
    "r2 = metrics.r2_score(y,y_pred)\n",
    "\n",
    "print(\"Results from sklearn.metrics:\\n\")\n",
    "print(\"R-Squared:\", r2)\n",
    "print()\n",
    "\n",
    "print(\"MAE:\",mae)\n",
    "print(\"MSE:\", mse)\n",
    "print(\"RMSE:\", rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A3: Gradient Boost Trees (Regression)\n",
    "Train the same way as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mae = metrics.mean_absolute_error(y, y_pred)\n",
    "mse = metrics.mean_squared_error(y, y_pred)\n",
    "rmse = np.sqrt(mse)  \n",
    "r2 = metrics.r2_score(y,y_pred)\n",
    "\n",
    "print(\"Ergebnisse von sklearn.metrics:\\n\")\n",
    "print(\"R-Squared:\", r2)\n",
    "print()\n",
    "\n",
    "print(\"MAE:\",mae)\n",
    "print(\"MSE:\", mse)\n",
    "print(\"RMSE:\", rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A3.1: Gradient Boost Trees (Regression) - additional Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "gbRegressor = GradientBoostingRegressor(n_estimators = 100, max_depth = 4,  max_leaf_nodes=50,  min_samples_leaf = 5)\n",
    "\n",
    "y_pred = cross_val_predict(gbRegressor, x, y, cv = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae = metrics.mean_absolute_error(y, y_pred)\n",
    "mse = metrics.mean_squared_error(y, y_pred)\n",
    "rmse = np.sqrt(mse)  \n",
    "r2 = metrics.r2_score(y,y_pred)\n",
    "\n",
    "print(\"Results from sklearn.metrics:\\n\")\n",
    "print(\"R-Squared:\", r2)\n",
    "print()\n",
    "\n",
    "print(\"MAE:\",mae)\n",
    "print(\"MSE:\", mse)\n",
    "print(\"RMSE:\", rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4 Interpretation of the quality criteria\n",
    "Interpret the quality criteria and make a statement on the usability of the models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.5 Train several models simultaneously and compare them visually\n",
    "Go through the code below and follow the individual steps. What does the visualization tell you?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "estimators = [('DT', DecisionTreeRegressor(min_samples_split = 10, min_samples_leaf = 5)),\n",
    "    ('RF', RandomForestRegressor(min_samples_split = 10, min_samples_leaf = 5, n_estimators = 100)), \n",
    "              ('GB', GradientBoostingRegressor(n_estimators = 100, max_depth = 4))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_regression_results(ax, y, y_pred, title, scores, elapsed_time):\n",
    "    \"\"\"Scatter plot of the predicted vs true targets.\"\"\"\n",
    "    ax.plot([y.min(), y.max()],\n",
    "            [y.min(), y.max()],\n",
    "            '--r', linewidth=2)\n",
    "    ax.scatter(y, y_pred, alpha=0.2)\n",
    "\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.get_xaxis().tick_bottom()\n",
    "    ax.get_yaxis().tick_left()\n",
    "    ax.spines['left'].set_position(('outward', 10))\n",
    "    ax.spines['bottom'].set_position(('outward', 10))\n",
    "    ax.set_xlim([y.min(), y.max()])\n",
    "    ax.set_ylim([y.min(), y.max()])\n",
    "    ax.set_xlabel('Measured')\n",
    "    ax.set_ylabel('Predicted')\n",
    "    extra = plt.Rectangle((0, 0), 0, 0, fc=\"w\", fill=False,\n",
    "                          edgecolor='none', linewidth=0)\n",
    "    ax.legend([extra], [scores], loc='upper left')\n",
    "    title = title + '\\n Evaluation in {:.2f} seconds'.format(elapsed_time)\n",
    "    ax.set_title(title)\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(9, 5))\n",
    "axs = np.ravel(axs)\n",
    "\n",
    "for ax, (name, est) in zip(axs, estimators):\n",
    "    start_time = time.time()\n",
    "    score = cross_validate(est, x, y,\n",
    "                           scoring=['r2', 'neg_mean_absolute_error'],\n",
    "                           n_jobs=-1, verbose=0)\n",
    "    elapsed_time = time.time() - start_time\n",
    "\n",
    "    y_pred = cross_val_predict(est, x, y, n_jobs=-1, verbose=0)\n",
    "\n",
    "    plot_regression_results(\n",
    "        ax, y, y_pred,\n",
    "        name,\n",
    "        (r'$R^2={:.2f} \\pm {:.2f}$' + '\\n' + r'$MAE={:.2f} \\pm {:.2f}$')\n",
    "        .format(np.mean(score['test_r2']),\n",
    "                np.std(score['test_r2']),\n",
    "                -np.mean(score['test_neg_mean_absolute_error']),\n",
    "                np.std(score['test_neg_mean_absolute_error'])),\n",
    "        elapsed_time)\n",
    "\n",
    "plt.suptitle('Performance Single predictors')\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional exercise - using trees as classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfCopy = dfPrepared.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfCopy['high revenue'] = np.where(dfCopy['total_sum'] > 300, 1, 0)\n",
    "dfCopy.drop('total_sum', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfCopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import accuracy_score, recall_score\n",
    "\n",
    "\n",
    "X = dfCopy.drop('high revenue', axis=1)\n",
    "y = dfCopy['high revenue']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify = y, random_state=42)\n",
    "\n",
    "clf = DecisionTreeClassifier(min_samples_split=10, min_samples_leaf=5, max_depth=10)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "tpr = recall_score(y_test, y_pred)  \n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"True Positive Rate (Recall):\", tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, recall_score\n",
    "\n",
    "X = dfCopy.drop('high revenue', axis=1)\n",
    "y = dfCopy['high revenue']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100, min_samples_split=10, min_samples_leaf=5, max_depth=10, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "tpr = recall_score(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"True Positive Rate (Recall):\", tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, recall_score\n",
    "\n",
    "X = dfCopy.drop('high revenue', axis=1)\n",
    "y = dfCopy['high revenue']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)\n",
    "\n",
    "clf = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=10, min_samples_split=10, min_samples_leaf=5, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "tpr = recall_score(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"True Positive Rate (Recall):\", tpr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
